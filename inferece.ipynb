{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecb43f6f-1456-4b75-8c2a-35620b9f9a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from llama_model import *\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5283099-b777-4cdf-8fa7-64186df2cc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"pythainlp/thainer-corpus-v2-base-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45a0ef37-5d8c-44e0-a892-47b123d13568",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model\n",
    "dim = 288\n",
    "n_layers =  6\n",
    "n_heads =  6\n",
    "n_kv_heads = n_heads\n",
    "multiple_of = 32\n",
    "dropout = 0.1\n",
    "max_seq_len = 350\n",
    "model_args = ModelArgs(\n",
    "    dim=dim,\n",
    "    n_layers=n_layers,\n",
    "    n_heads=n_heads,\n",
    "    n_kv_heads=n_heads,\n",
    "    vocab_size=tokenizer.vocab_size ,\n",
    "    multiple_of=multiple_of,\n",
    "    max_seq_len=max_seq_len,\n",
    "    dropout=dropout,\n",
    ") \n",
    "model = Transformer(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "270a5365-036d-44f4-b286-90713482cbda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (tok_embeddings): Embedding(25005, 288)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x TransformerBlock(\n",
       "      (attention): Attention(\n",
       "        (wq): Linear(in_features=288, out_features=288, bias=False)\n",
       "        (wk): Linear(in_features=288, out_features=288, bias=False)\n",
       "        (wv): Linear(in_features=288, out_features=288, bias=False)\n",
       "        (wo): Linear(in_features=288, out_features=288, bias=False)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (w1): Linear(in_features=288, out_features=768, bias=False)\n",
       "        (w2): Linear(in_features=768, out_features=288, bias=False)\n",
       "        (w3): Linear(in_features=288, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (attention_norm): RMSNorm()\n",
       "      (ffn_norm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (output): Linear(in_features=288, out_features=25005, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load('best_train_loss_model.pth'))\n",
    "# model.load_state_dict(torch.load('best_val_loss_model.pth'))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc7789cd-6d97-4da5-aefa-0f26084097ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_function(model, tokenizer, prompt, max_new_tokens=50, temperature=0.2, top_k=10,):\n",
    "\n",
    "    tokenized_prompt =  tokenizer.encode(prompt)[1:-1]\n",
    "    tokenized_prompt = (torch.tensor(tokenized_prompt, dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "    generated_tokens = []\n",
    "    context_tokens = tokenized_prompt\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        context_tokens = context_tokens[:, -min(model.params.max_seq_len, context_tokens.size(1)):]\n",
    "\n",
    "        output = model(context_tokens)\n",
    "        logits = output[:, -1, :]\n",
    "        logits = logits / temperature\n",
    "        v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "        logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        context_tokens = torch.cat((context_tokens, next_token), dim=1)\n",
    "\n",
    "        generated_tokens.append(next_token.item())\n",
    "\n",
    "    # Decode the generated tokens to text\n",
    "    generated_text = tokenizer.decode(generated_tokens)\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1141acc9-3b68-4366-a1aa-bc4521623f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "สื่อข่าวคุณเจ้าหน้าการปกครองผลท่าทีท่าทีท่าทีท่าทีในการกรณีกรณีกระแสกรณีกระแสความรู้สึกกระแสความรู้สึกความรู้สึกความรู้สึกความรุนแรงเหตุการณ์เหตุการณ์เหตุการณ์เรื่องเรื่องเรื่องเรื่องเรื่องเรื่องเรื่องเพศเพศเพศแรงงานเพศเพศแรงงานเพศเพศแรงงานเพศท้องถิ่นเพศท้องถิ่นเพศท้องถิ่นปเพศเพศ\n"
     ]
    }
   ],
   "source": [
    "prompt = \"เราสามารถจะใช้กูเกิ้ลค้นหาอะไร\"\n",
    "generated_text = generate_text_function(model, tokenizer, prompt)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9becbb8-2fce-426e-ae3a-abbe9c32bed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "เรียกร้องคณะกรรมการ องค์กรปฏิรูป ปฏิรูปกฎหมายกฎหมายกฎหมายกฎหมาย กฎหมาย  พพพพ<unk>กพธ!&'ลางการยกเลิกการยกเลิกยศมหาวิทยาลัร้าว...........      พ. \n"
     ]
    }
   ],
   "source": [
    "prompt = \"ศาลอุทธรณ์\"\n",
    "generated_text = generate_text_function(model, tokenizer, prompt)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8e65f8b-9235-4fb7-87d1-80d92a46c819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>ประสบปัญหาก <unk> พ พ นพo พ พ...........      พ พ  \"พ  \" \" \" พ พ \n"
     ]
    }
   ],
   "source": [
    "prompt = \"เมืองหลวงของประเทศไทย\"\n",
    "generated_text = generate_text_function(model, tokenizer, prompt)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a44d175-ed1b-4d73-8a88-6d78ba0587ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ก <unk> พ เวลา นนพ ที่พ \"พ  \"พ.......     นพ พ พ.  พ พ..   \" \"\n"
     ]
    }
   ],
   "source": [
    "prompt = \"ประเทศไทยใช้ภาษาอะไร\"\n",
    "generated_text = generate_text_function(model, tokenizer, prompt)\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
